# /!\ -- Do not erase!! -- /!\
hydra:
  job:
    name: "${name}"
  run:
    dir: "${rundir}/${name}"
  sweep:
    dir: "${dir:runtime}/saves/${name}"
    subdir: "${base:${data.dataset.root}}"

# Run name
name: ???
# Run path
rundir: "${dir:runtime}/saves/${name}"

# Defaults configuration, that can be overriden in commande line arguments
defaults:
  - data:
  - data:
    - dgt-1024-mag
  - model:
    - encoder/vae_conv_large
    - decoder/vae_conv_large
  - callbacks: audio_defaults
  - _self_

data:

  # Data Augmentations
  augmentations:
    - {type: Shift, args: {prob: 0.5}}
    - {type: Normal, args: {prob: 0.5, amp_range: 0.1, clamp: [0, none]}}

  loader:
    batch_size: 256
    shuffle: True

# here we add specific parameters for the decoder's output (softplus + normal distribution).
model:
  type: AutoEncoder
  latent:
    dist: Normal
    dim: 8
  decoder:
    args:
      # we fill the out_nnlin to enforce the positivity of the decoder output
      out_nnlin: Softplus
      target_dist: none
      # if you learn through LogDensity, you will need a normal output.
      # in this case, comment l.59 and uncomment l.62
      # target_dist: Normal 
  training: 
    # regularization term trade off
    beta: 1.0
    # warmup steps
    warmup: 5e5
    # specify if warmup is in training batches or epochs (batch / epoch)
    beta_schedule_type: batch 
    # optimizer arguments
    optimizer: 
      type: Adam
      args:
        lr: 1.e-5
    # specify reconstruction loss
    reconstruction:
      type: SpectralConvergence
      # type: MSE
      # type: LogDensity 
    # specify regularization loss
    regularization:
      type: KLD
      args:
        # free bits prevents catastrophic mode collapse. 
        # be careful though, target value depends on the 
        #   latent dimensionality!
        free_bits: 10

pl_trainer:
  max_epochs: 2000
  # limit_train_batches: 1
  # limit_val_batches: 1

